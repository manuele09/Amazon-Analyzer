{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3002ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880a58b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'findspark' from '/home/emanuele/.local/lib/python3.9/site-packages/findspark.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c545e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_host=\"http://elasticsearch:9200\"\n",
    "elastic_index=\"taptweet\"\n",
    "kafkaServer=\"kafkaServer:9092\"\n",
    "topic = \"tap\"\n",
    "es = Elasticsearch(hosts=elastic_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f198f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6510a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/emanuele/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/emanuele/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/emanuele/.ivy2/cache\n",
      "The jars for the packages stored in: /home/emanuele/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bbf84737-8756-4f2a-8f88-15c72e2c6bdf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.4.4 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      ":: resolution report :: resolve 615ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.4.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bbf84737-8756-4f2a-8f88-15c72e2c6bdf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/22ms)\n",
      "22/06/16 18:28:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a286168",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tp.StructType([\n",
    "    tp.StructField(name= 'id', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'product', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'title', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'rating', dataType= tp.FloatType(),  nullable= True),\n",
    "    tp.StructField(name= 'body', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'date', dataType= tp.StringType(),  nullable= True)\n",
    "])\n",
    "\n",
    "set = spark.read.csv('reviews.csv',\n",
    "                         schema=schema,\n",
    "                         header=True,\n",
    "                         sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee08e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.show(2)\n",
    "set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('body') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "sequenceClassifier = DistilBertForSequenceClassification \\\n",
    "      .load(\"/home/emanuele/Tap_Project/spark_nlp/cache_pretrained/distilbert_base_sequence_classifier_amazon_polarity_en_3.3.3_3.0_1637503776952\") \\\n",
    "      .setInputCols(['token', 'document']) \\\n",
    "      .setOutputCol('class') \\\n",
    "      .setCaseSensitive(True) \\\n",
    "      .setMaxSentenceLength(512)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    sequenceClassifier    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(set).transform(set)\n",
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(set).transform(set).select(\"body\", \"class\")\n",
    "result = result.withColumn(\"valutazione\", F.col(\"class.result\"))\n",
    "result = result.drop(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
